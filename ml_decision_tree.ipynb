{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "* Comes under Supervised Learning\n",
    "* Used for both Classification and Regression\n",
    "* Leaves represent class labels\n",
    "* Branches represent conjunction of feature characteristics leading to class labels\n",
    "* Uses simple trick to make non-linear decision making by using simple linear decision surface\n",
    "    * Allows you to ask multiple linear questions one after another\n",
    "\n",
    "\n",
    "![Decision Tree](images/CART_tree_titanic_survivors.png)\n",
    "Image source: https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "\n",
    "### Types\n",
    "* Classification tree\n",
    "* Regression tree\n",
    "* Ensemble methods\n",
    "     * Boosted Trees\n",
    "     * Bagging\n",
    "     * Random Forest\n",
    "     * Rotation Forest\n",
    "\n",
    "### Pros\n",
    "* Simple to understand and interpret\n",
    "* Able top handle both numeric and categorical data\n",
    "* Requires little data preparation\n",
    "* Uses white box model\n",
    "* Possible to validate model using statistical tests\n",
    "* Performs well with large datasets\n",
    "* Mirrors human decision making\n",
    "\n",
    "### Cons\n",
    "* Not as accurate as other approaches\n",
    "* Easy to overfit if not taken care\n",
    "* Can be very non-robust (little change in training data can cause huge change in model, hence prediction)\n",
    "* For categorical data, tree is biased towards those categories which have more levels in the tree\n",
    "\n",
    "### References\n",
    "* Udacity Intro to Machine Learning\n",
    "* https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "* Hands On Machine Learning, Chapter 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sklearn for using decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree # The Classifier\n",
    "from sklearn import datasets # The sample data\n",
    "from sklearn.model_selection import train_test_split # Helper to split data\n",
    "\n",
    "# Create the classifier, load the sample data\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "data = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.datasets.base.Bunch'>\n",
      "dict_keys(['target_names', 'images', 'DESCR', 'target', 'data'])\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect the data\n",
    "print(type(data))\n",
    "print(data.keys())\n",
    "print(type(data.data))\n",
    "print(type(data.target))\n",
    "print(data.data.shape)\n",
    "print(data.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84999999999999998"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict score\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87777777777777777"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try with various min_sample_split and criterion values\n",
    "clf = tree.DecisionTreeClassifier(min_samples_split=2, criterion='entropy')\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try out with some big datasets - the Labeled Faces in the Wild dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading LFW metadata: http://vis-www.cs.umass.edu/lfw/pairsDevTrain.txt\n",
      "Downloading LFW metadata: http://vis-www.cs.umass.edu/lfw/pairsDevTest.txt\n",
      "Downloading LFW metadata: http://vis-www.cs.umass.edu/lfw/pairs.txt\n",
      "Downloading LFW data (~200MB): http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.datasets.base.Bunch'>\n",
      "dict_keys(['target_names', 'images', 'DESCR', 'target', 'data'])\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(13233, 2914)\n",
      "(13233,)\n"
     ]
    }
   ],
   "source": [
    "data = datasets.fetch_lfw_people()\n",
    "# Let's inspect the data\n",
    "print(type(data))\n",
    "print(data.keys())\n",
    "print(type(data.data))\n",
    "print(type(data.target))\n",
    "print(data.data.shape)\n",
    "print(data.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, train_size=0.7, random_state=42)\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "* Controls how a DT decides where to split the data\n",
    "* Measure of impurity in a bunch of examples\n",
    "![Entropy](images/Entropy_3.png)\n",
    "Image Source: http://www.saedsayad.com/images/Entropy_3.png\n",
    "* pi is is fraction of examples in class i\n",
    "* Entropy is opposite of purity\n",
    "* If all examples are of same class, Entropy will be 0\n",
    "* If examples are evenly split between all classes, Entropy will be 1\n",
    "* Lower entropy points toward more organized data, and that a decision tree uses that as a way how to classify events.\n",
    "* Every node in the tree will have entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain\n",
    "\n",
    "![Information Gain](images/information_gain.jpg)\n",
    "Image source: Udacity Intro to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Concept to know: Bias Variance tradeoff\n",
    "    * An algorithm which is highly Biased does not learn from training data\n",
    "    * An algorithm which is high on variance is very sensitive to the training data\n",
    "    * One way or other, they do not have the capacity top adapt to new data\n",
    "    * This is Bias Variance tradeoff\n",
    "    * Tuning an ML Algorith means reaching a sweet spot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes from Victor's videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://www.youtube.com/playlist?list=PLBv09BD7ez_4_UoYeGrzvqveIR_USBEKD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hard to guess\n",
    "* Tries to _understand_ when an event occurs\n",
    "* The only algorithm which is not a black box - not hard to interpret, can be visualized\n",
    "* Divide and Conquer\n",
    "    * Split into subsets\n",
    "    * are they pure? (all yes or all no)\n",
    "    * if yes: stop\n",
    "    * if not: repeat\n",
    "* Algo: Take one feature, see if there is correlation between it's values and the labels.If there is 100% correlation (they are pure), stop. Else proceed to next feature\n",
    "* ID3 Algorithm builds the decision tree data structure\n",
    "    * Recursive\n",
    "    \n",
    "![ID3 Algorithm](images/id3_algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you decide which attribute is the best to split on?\n",
    "\n",
    "When to stop? What can be done so as not to overfit?\n",
    "\n",
    "GainRatio/SplitEntropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be used for multi class classification and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "* Easily interpretable\n",
    "* Easily handle irrelavent data\n",
    "* Can handle missing data\n",
    "* Very compact\n",
    "* Very fast at testing time: O(depth) - very fast after you have trained it\n",
    "\n",
    "\n",
    "Cons:\n",
    "* Only axis aligned splits of data\n",
    "* Greedy (may not fit the besttree)\n",
    "* Fastest Classifier you can build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the complexity of decision tree algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Decision Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instead of growing one decision tree on your data, grow K different decision trees:\n",
    "    * Pick a random subset S<sub>r</sub> of training example\n",
    "    * Grow a fill ID3 tree (no prunning)\n",
    "        * When splitting, pick from d << D random attributes\n",
    "        * Compute gain based on S<sub>r</sub> instead of full set\n",
    "    * repeat for r = 1 ...K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://www.youtube.com/watch?v=U2A-g6-Prrs&feature=youtu.be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

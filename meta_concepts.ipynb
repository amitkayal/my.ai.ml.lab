{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of concepts in AI, ML, Probablity, Statistics\n",
    "anandsaha@gmail.com / teleported.in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specificity and Sensitivity\n",
    "\n",
    "Ref:\n",
    "* https://www.youtube.com/watch?v=21Igj5Pr6u4\n",
    "* https://www.youtube.com/watch?v=vtYDyGGeQyo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes explanation\n",
    "\n",
    "Ref:\n",
    "* http://stackoverflow.com/questions/10059594/a-simple-explanation-of-naive-bayes-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression vs Classification\n",
    "\n",
    "In regression, we try to predict a real number (continuous)  \n",
    "In classification, we try to predict a category (from a set of possible finite categories. Discrete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "* Purpose is to discover sub population of the population\n",
    "* Useful to identify outliers (Classification can not do that)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary\n",
    "For Classification, the bounary within the problem space (the space of all possible feature values) which our classifier learns which separates one class from another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute Value Pair\n",
    "* The way we represent data in machine learning\n",
    "* We need to represent observations numerically\n",
    "* You can be very creating in designing how to represent your data in Machine Learning\n",
    "* Highly unlikely one will come up with a new ML algorithm. But one can show creativity in how to represent data.\n",
    "* Once can achieve gain in performance by engineering the right representation of data.\n",
    "* Each individual obsevation represented with a set of attributes, and each attribute has a number assigned to it:\n",
    "```python\n",
    "person = {height_inch: 70, weight_kg: 85, age_years: 45}\n",
    "```\n",
    "\n",
    "* Unordered _bag-of-features_\n",
    "* Any data needs to be converted to this form before learning takes place\n",
    "* Attributes can be categorised into:\n",
    "    * Categorical: Red, Blue, Yellow \n",
    "        * Discrete list of possible values - no ordering\n",
    "        * Can only hold one value at a time (out of the many categories)\n",
    "        * Mutually exclusive\n",
    "        * Categories turn to numbers when you code it up\n",
    "        * But not really numbers, can only do == and !=\n",
    "        * Tagging process is important, needs to be controlled\n",
    "    * Ordinal: high, medium, low \n",
    "        * Like categorical, but has natural ordering to it\n",
    "        * Encoded as numbers, can test for ordering in addition to equality check\n",
    "        * ==, !=, >, <, >=, <= etc.\n",
    "    * Numerical: 1, 2, 3 \n",
    "        * Meaningful to add, multiply, computer mean/variance\n",
    "        * Usually you want to normalize them\n",
    "        * Because raw incoming data can take any range, need to control it\n",
    "        ```python\n",
    "        x` = (x - mean)/st.dev # Will make the values center around 0, with variance of 1\n",
    "        x` = (x-min)/(max-min) # Makes sense if there are definite max and min values\n",
    "        ```\n",
    "        * You bring the data to the same scale, so that all units are roughly comparable to each other\n",
    "        * They are sensitive to outliers - unusually large or small values. Must be handled before normalization. Normalization will get confused by outliers.\n",
    "        * BUT - Some outliers might actually be valid data due to skewed distribution\n",
    "            * Systematic extreme values\n",
    "            * Affects regression, kNN, NB. Not DT's\n",
    "            * Fix: log(x) for positive numbers or atan(x) for pos and neg numbers, then normalise\n",
    "            * Can also use Cumulative Distribution Function (Use rank instead of values)\n",
    "        * Monotonic vs Non-monotonic: \n",
    "            * Monotonic e.g.: higher net worth -> lower lending risk\n",
    "            * Non-monotonic: Age -> winning marathon\n",
    "            * Fix: Quantization (Turn numeric to Categorical or Ordinal)\n",
    "                * Can be unsupervised, overlapping\n",
    "    * Time series: series of tweets\n",
    "* Picking attribute (Detecting preditor in image) - one idea that works is similarity\n",
    "    * If you have several instances/observations and they have the same class\n",
    "* Blurring helps Machine Learning (recognising numbers, letters)\n",
    "\n",
    "Ref:\n",
    "* https://www.youtube.com/playlist?list=PLBv09BD7ez_4Z5ap8fJOzn-Ezz7WpWI5-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case study: Identifying a Zebra\n",
    "* In this domain, using pixel won't work - a single pixel no longer carries any meaning\n",
    "    * A zebra can be put upside down and still be zebra\n",
    "* **Segment** the image into regions - goes into image and identifies homogenous regions - regions which has similar local properties\n",
    "    * Algorithms: BlobWorld, Normalized Cuts\n",
    "    * Caveat: Segmentation algorithms not so much accurate on still pics, good on videos\n",
    "    * Hope that the anomalies would be systematic, so that it happens in every photo\n",
    "* Compute features describing the region\n",
    "    * position\n",
    "    * relative area\n",
    "    * circumference\n",
    "    * convexity\n",
    "    * orientation\n",
    "    * color frequency\n",
    "    * texture filters etc.\n",
    "   \n",
    "### Case study: text classification - spam no spam\n",
    "* Input: string of characters\n",
    "* Idea: Words carry meaning\n",
    "* Naive way: Words as value (as many values as words in email, categorical)\n",
    "* For robustness, instead of tying a word to a position in the email, tie it to a word in the vocabulary\n",
    "* May use frequency or tf-idf weights\n",
    "* Use binary values for attributes\n",
    "\n",
    "### Case study: Music Classification\n",
    "* Naive representation\n",
    "    * sample at regular intervals\n",
    "    * X(t) = amplitude at t\n",
    "* Else (better)\n",
    "    * Music is a time series\n",
    "    * Decompose it into constituent base frequencies f (Fourier transform)\n",
    "    * Find weight of each f\n",
    "    * The weights of the base frequency can be used to classify music - it is insensitive to shifts and volume increase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency, TF-IDF weights\n",
    "* Word Frequency\n",
    "    * How many times the word occured in the document\n",
    "* TF-IDF\n",
    "    * TF: Term Frequency, the number of times the word occured in the document\n",
    "    * IDF: Inverse Document Frequency, words unique to this document, but rarely found in corpus of other documents are important for this document\n",
    "    * Score = #times the word appeares in this document * log(# total documents / # other documents the term appears in)\n",
    "    * Example: If the word 'automobile' appeared 10 times in this document, if there are 100 documents in the corpus, and if 20 other documents have the word 'automobile' in them, then:\n",
    "        * `Score = 10 * log(100/20) = 6.98`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness of an algorithm\n",
    "A small change to input should result in small change in output (and not huge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Algorithms and Predictors\n",
    "* Learning Algorithms create Predictors (functions) that can be then deployed in production\n",
    "* Learning Algorithms use training data (sample inputs and expected outputs) to create the Predictor\n",
    "* For Classification: Predictor takes the form of a decision boundary. The function creating the decision boundary _is_ the Predictor. And the learning algorithm creates it (the function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised vs Unsupervised\n",
    "\n",
    "* Supervised\n",
    "    * Trying to predict a specific quantity\n",
    "    * Have training examples with lables\n",
    "    * Can measure accuracy directly\n",
    "    \n",
    "* Unsupervised\n",
    "    * Trying to understand data\n",
    "    * Looking for structure or unusual patterns\n",
    "    * Not looking for something specific\n",
    "    * Does not require labled data\n",
    "    * Evaluation usually indirect or qualitative\n",
    "* Semi supervised\n",
    "    * Use Unsupervised methods to improve supervised algos\n",
    "    * Usually few labled examples + lots of unlabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of learning\n",
    "1. Classification (Supervised)\n",
    "    * Multiclass Classification\n",
    "        * Classes mutually exclusive\n",
    "        * NB, kNN, DT, Logistics\n",
    "        * Assumes that there are not other classes than specified (i.e. classes are predefined)\n",
    "    * Binary Classification\n",
    "        * SVM is fundamentally a binary classifier\n",
    "        * One vs Rest can be done\n",
    "            * {a} vs {not b}, {b} vs {not b}\n",
    "        * Classes may overlap\n",
    "            * Instances can be both a and b\n",
    "            * Can be in none of the classes\n",
    "        * E.g. SVM, Logistic (softmax), Perceptron\n",
    "2. Regression (Supervised)\n",
    "3. Clustering (Unsupervised)\n",
    "4. Dimentionality Reduction (Unsupervised)\n",
    "\n",
    "\n",
    "\n",
    "* Linear Classifier: Draws straignt decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification accuracy and imbalanced classes\n",
    "* Example: Look at a paper and predict if it will lead to a Nobel Prize\n",
    "    * Always say 'no', will lead to 99.9% accuracy classifier\n",
    "    * This is an example of unbalanced classes, where one class dominates\n",
    "    * Question to ask: What is the right error metric?\n",
    "    * Give relative weights to false positives, false negatives\n",
    "* Accuracy/Error rate poor metric (above)\n",
    "* Want:  cost(Misses) > cost (FA)\n",
    "    * What is FA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative vs Discriminative learning\n",
    "* Generative approach\n",
    "    * Probabilistic \"model\" of each class\n",
    "    * Decision boundary\n",
    "        * Where one model becomes more likely\n",
    "    * Natural use of unlabeled data, can also use labeled data\n",
    "    * Supervised or Unsupervised\n",
    "    * All Generative are Probablistic, but all Probablistic are not Generative, i.e. for some, you can calculate the probablity without first creating a model\n",
    "* Discriminative\n",
    "    * Focus on decision boundary\n",
    "    * More powerful when we have lots of examples\n",
    "    * Not designed to use unlabeled data\n",
    "    * Only Supervised tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "* Isolated instances in your data that don't look right\n",
    "* E.g. extreme values for one or more attribute value\n",
    "* Can be detected using confidence intervals\n",
    "* Remove or threshold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

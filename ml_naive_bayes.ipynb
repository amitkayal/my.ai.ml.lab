{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: \n",
    "* https://www.youtube.com/playlist?list=PLBv09BD7ez_6CxkuiFTbL3jsn2Qd1IU7B\n",
    "* https://www.youtube.com/watch?v=ivBSZZyaRHY&feature=youtu.be\n",
    "\n",
    "Goal: learning function `f(x) -> y`\n",
    "\n",
    "Where\n",
    "* `y` is one of k classes (e.g. spam/ham, digit 0-9)\n",
    "* x = x<sub>1</sub>...x<sub>d</sub> - values of attributes (numeric or categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Classifier\n",
    "* Part of Bayesian Classifiers\n",
    "    * Part of Probablistic Classifiers - Assign class based on computed probablity of that class. First calculates the probablity, then selects the class which has the highest probablity given the observation\n",
    "    \n",
    "`y_hat = arg max_y P(y|x)`\n",
    "\n",
    "y_hat is the predicted class\n",
    "\n",
    "* Good for multi class prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Probablity of a Class:\n",
    "\n",
    "![Bayesian Classification](images/bayesian_classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Probablity of `y (spam)` given `x (email)` equals to\n",
    "    * Probablity of the class `P(y)` (spam let's say) - `prior (this is independent of the email`\n",
    "    * Multiplyed by probablity of that particular email `x (spam)` occuring given the class `y` - `class model (assuming the email is spam, what is the combination of words that we should expect occuring and not occuring)`\n",
    "    * Divided by probablity of `x` - `normalizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Further, consider an example. Given symptoms (cough, temperature, skin paleness etc.), we want to predict if a patient has Ebola or not\n",
    "    * In this case, `P(y)` is prior - i.e. without knowing anything about the patient, what is the probablity that he has Ebola\n",
    "        * Encodes which class are common, which class are rare\n",
    "        * Apriori much more likely to have other disease than Ebola\n",
    "    * `P(x|y)` - class model - given that he has Ebola, what is the likelyhood that he will exhibit symptoms `x`\n",
    "    * `normalizer` - usually left out because it doesn't affect the selection of class - it's a constant\n",
    "        * Normalize probablities across observations\n",
    "        * It affects the calculation if your question is: Who is more at risk amongst all the patients? - bumps up posterior probablity, normalizes the probablities to bring them on the same scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Naive Bayes is a generative model\n",
    "* A complete probablity distribution for each class\n",
    "    * Defines likeluhood for any point x\n",
    "    * P(y|x) proportional to P(x|y) * P(y)\n",
    "    * P(class) via P(observation) \n",
    "    * Can _generate_ synthetic observation\n",
    "* All generative classifiers are probabilistic, but all probabilistic classifiers are not generative\n",
    "    * i.e. in these cases, one can generate the probablity directly without generating a model for each class. E.g. logistic regression\n",
    "    * \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bayes Formula](images/bayes_formula.png)\n",
    "![Bayes Classifier](images/bayes_classifier.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Base Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generative Model - Classifies by first generating models of each class\n",
    "* Normal Baysian approach does not scale if number of features are huge\n",
    "* E.g. if each observation is characterised by 3 binary features, then the total number of possible types of observations are 2<sup>3</sup>\n",
    "* Computationally intensive, also model gets copmplex - overfitting\n",
    "\n",
    "Naive:\n",
    "\n",
    "* Instead of assuming that the features in an observation are dependent (`2^n`), assume that they are independent of each other. As a result:\n",
    "    * `P(a, b) = P(a) * P(b)`\n",
    "\n",
    "* Independence of events makes it Naive\n",
    "* Compute `P(x1...xd|y)` for every observation x1...xd\n",
    "    * class-conditional `counts`, based on training data\n",
    "    * problem: may not have seen every x1...xd for every y\n",
    "        * digits: 2<sup>400</sup> possible black/white patterns (20x20)\n",
    "        * spam: every possible combination of words: 2<sup>10000</sup>\n",
    "    * often have observations for individual x<sub>i</sub> for every class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independence Assumption\n",
    "![Independence Assumption](images/independence_assumption.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional Independence:\n",
    "* `P(Beach, Stroke) > P(Beach) P(Stroke)`\n",
    "* But, If we argue that it's the heat which is causing the stroke:\n",
    "\n",
    "    `P(Beach, Stroke|Heat) == P(Beach|Heat) P(Stroke|Heat)`\n",
    "* In Classification: Class Value explains all the dependence between attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We use Classical Naive Bayes when the features  are categorical\n",
    "* We use Gaussian Naive Bayes when the features are continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where can NB go wrong?\n",
    "* If the distribution of the observations is such that the mean and variance are same - i.e. no way to differentiate them\n",
    "    * One could have take advantage of corelation, but NB cannot do coorelation\n",
    "* NB assumes independent occurances, hence can mis classify at times\n",
    "* Zero frequency problem\n",
    "    * Zipf's law: will happen with half of your attributes\n",
    "* Missing data (easy with Naive Bayes):\n",
    "    * Ignore attribute in instance where value is missing\n",
    "    * Compute likelihood based on observed attributes\n",
    "    * no need to fill in or explicititly model missing values\n",
    "    * based on conditional independence between attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X, y = datasets.make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X1 = []\n",
    "X2 = []\n",
    "for x in X:\n",
    "    X1.append(x[0])\n",
    "    X2.append(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(X1, X2, c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf = naive_bayes.GaussianNB()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if hasattr(clf, \"decision_function\"):\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "else:\n",
    "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "Z = Z.reshape(xx.shape)\n",
    "ax1.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://www.youtube.com/playlist?list=PLBv09BD7ez_50pj5kcKFYee7QPcg3ImCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The purpose of building predictors is to predict on future data\n",
    "* We want to do well on future data\n",
    "    * With training data, you can do well - with DT, Kernels and KNN you can get 100% accuracy - does not mean you will do well on future data\n",
    "    * Can fit idiosyncrasies of our training data\n",
    "* Overfitting - when you create a predictor which fits your training data too well\n",
    "    * Predictor too complex and flexible\n",
    "    * Fits noise in the training data\n",
    "    * Patterns that will not reappear\n",
    "* A Predictor F over fits the data, if we can find another predictor F<sup>'</sup> such that F<sup>'</sup> makes more mistakes on training data than on unseen data, compared to F\n",
    "* Predictor is under fitting if it is too simple \n",
    "* A Predictor F under fits the data, if we can find another predictor F<sup>'</sup> such that F<sup>'</sup> gives better accuracy and less error on both training and unseen datasets, compared to F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Under Over Fit](images/under_over_fit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Algorithms will have knobs or parameters which can be tuned to fine tune fitting \n",
    "    * Regression: Order of the polynomial\n",
    "    * NB: Number of attributes, limits on variance, epsilon\n",
    "    * DT: # nodes in the tree, pruning confidence\n",
    "    * kNN: number of nearest neighbours\n",
    "    * SVM: Kernel type, cost parameters\n",
    "* We need knobs because various predictors will need different flexibility for the problem domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training Error (measure errors in training data)\n",
    "* Generalization Error (measure errors in unseen data)\n",
    "    * Don't know what the classes or features will be\n",
    "    * But we know what \"range\" it will have, i.e. {x, y}\n",
    "        * e.g. for classification, x: all possible 20x20 black/white bitmaps\n",
    "        * y:{0..9} digits\n",
    "        \n",
    "Estimating Generalization Error \n",
    "\n",
    "![Gen Error](images/generalization_error.png)\n",
    "\n",
    "* Set aside a subset of training data for testing\n",
    "* Learn a predictor without using any of the testing data\n",
    "* Calculate error over testing dataset\n",
    "    * Gives an estimate of true generalization error - depends on how representative the testing data is of the future data\n",
    "    * The more data you have in the testing set, the more closer the testing error will be to generalisation error\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confidence interval of the future\n",
    "* What range or errors can we expect for future test sets?\n",
    "    * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incomplete ----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
